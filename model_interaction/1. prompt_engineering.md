# Introduction to Prompt Engineering
We have seen what ***prompts*** are and why it is so important in the Generative AI field. Now we will be discussing what exactly is ***Prompt Engineering***, and what are the vwrious prompting tenchniques that are used. 
### Definition
Prompt Engineering is the practice of designing effective inputs (prompts) to guide large language models (LLMs) like GPT/Claude to produce desired outputs.

Prompt Engineering is an emerging field focused on crafting and refining inputs to effectively harness the capabilities of large language models (LLMs) across a broad range of applications.This skill set empowers practitioners to explore both the strengths and limitations of LLMs. For researchers, it serves as a way to boost safety and performance on tasks like reasoning and Q&A. Developers leverage prompt engineering to build reliable and efficient interfaces between language models and external systems or workflows.

This guide is designed to offer the theoretical understanding of Prompt Engineering, helping you get the most out of LLMs through smart, optimized prompting.

---

# Basics of Prompting
### Prompting an LLM
Even simple prompts can produce impressive results, but the output quality largely depends on how clearly the prompt is written and how much useful information it includes. A well-structured prompt typically consists of the task or question along with supporting details such as background context, input data, or illustrative examples. These components help guide the model more precisely and significantly enhance the relevance and accuracy of its responses.

Let’s begin by looking at a basic example of a straightforward prompt:

Prompt:

> The sky is

Output:

> blue.

As seen in the example above, the language model generates a string of tokens that follow logically from the phrase "The sky is." However, the result may not always align with your intended task or goal. This simple case illustrates the importance of including clearer context or more specific instructions to guide the model’s behavior effectively. That’s the essence of prompt engineering—shaping your inputs to produce more accurate and useful outputs.

Let’s refine the prompt a little:

Prompt:

> Complete the sentence: 
The sky is

Output:

> blue during the day and dark at night.

Looks better, right? In this revised prompt, you’re clearly guiding the model to complete a sentence, and the output aligns much more closely with your intent. This kind of precise instruction is the core idea behind prompt engineering—crafting inputs that clearly communicate the task you want the model to perform.

This simple example barely scratches the surface of what modern LLMs can do. Today’s models are capable of handling a wide range of complex tasks, from summarizing lengthy articles to solving math problems and writing code.

---

# Prompt Formatting
A typical prompt can either be a question:

> What is Deep Learning?

Or an instruction:

> Explain artificial intelligence in simple terms.

You can also use Q&A format, which is useful in many tasks:

> Q: What is Deep Learning?
> A:

This is called zero-shot prompting, where you give no examples, just a direct task.
