# Fine Tuning an LLM
In this blog we are going to cover the folowing topics:

1. What is Fine Tuning?
3. Why do we need Fine Tuning?
4. When and When NOT to use Fine Tuning?
5. Types of Fine Tuning
6. Hands on pactice

---

# What is Fine Tuning?
Suppose you have a pre-trained model like GPT-4 or GPT-3.5. Now imagine this model as a super-smart student who has already read the entire internet — but has never read your specific textbook.

Let’s say you want this student to fully understand your book — whether it's on medicine, engineering, or any other specialized domain.

Now you have two options:

Option A: You teach them everything about medicine from scratch.
Option B: You simply teach them your book to help them grasp your specific content.

If you ask me, I’d go with Option B — because this student already knows a lot about medicine from the internet. You don’t need to start over. You’re just giving them your specialized context.

This saves your time, energy, and the student’s effort as well.

In the same way, when we want GPT-4 to become an expert in law and legal systems, we don’t need to retrain it from scratch on massive legal data. Instead, we fine-tune it on a smaller, domain-specific dataset, which gives it the additional understanding it needs — saving time, compute, and cost.

**Fine-tuning is taking a pre-trained model and training it a little more on your custom dataset so it performs better for your task.** During fine-tuning, the model's internal parameters — its weights and biases — are updated based on your data. This is the same principle by which all deep learning models learn. Example: ChatGPT - fine tuned on the top GPT base models using carefully curated conversational data, enabling it to better understand and respond like a helpful assistant.

---

# Why do we need Fine Tuning?
Now you might be wondering that we can make the LLM to act as an expert in Law-practices with the help of prompting, so why do we need to fine tune specifically. Well here's the answer to your "why". We will understand this with the help of a story.

## Prompting vs Fine-Tuning — What’s the Difference?

Imagine you’ve hired a brilliant AI student — let’s call her Maya. Maya has read all the books on the internet, seen every YouTube tutorial, and studied every Wikipedia article. She's super smart. Now, you want Maya to help you write medical reports for a hospital.

**Day 1: You try prompting (giving her instrauctions)**
You say: “Hey Maya, every time I give you patient data, write a report in this exact format. Include vitals, diagnosis, suggested tests, and > always end with ‘Reviewed by Dr. Sharma’.” Maya nods and does a decent job. But the next day, you give her a new case, and she forgets the format. She forgets to mention the doctor's name. Sometimes she writes in bullet points. Sometimes in full paragraphs. So you keep repeating the same instructions: “Format like this. End with Dr. Sharma. Use technical terms.” It’s like writing her a sticky note every single time.

**Day 5 — You're Tired**
You're exhausted. You’ve written the same long prompt 50 times. And sometimes Maya still messes up the style or forgets medical abbreviations. You think: "She’s brilliant — but I wish she’d just learn how we do things here.” So You Fine-Tune Her (train her a bit). You spend a weekend with Maya. You show her hundreds of past medical reports from your hospital. You walk her through the patterns, the terminology, the format. She studies your data closely. You don’t need to teach her everything about medicine — she already knows that. You’re just showing her how you want things done. This is fine-tuning. Now, Maya doesn’t need sticky notes anymore.

**Week Later — She’s Fluent**
Now Maya produces flawless medical reports in seconds. She uses your preferred vocabulary, follows your templates, and never forgets “Reviewed by Dr. Sharma.”

She understands the vibe. She’s customized. She's fast. And you no longer need to remind her how to behave.

### Moral of the Story

***Prompting*** is like giving ***instructions*** to a brilliant but forgetful assistant — works for short-term, but not always reliable.

***Fine-tuning*** is like ***training*** that assistant to work your way, so she just knows what to do — permanently.

So even though Maya is a genius who read the whole internet, she still needs fine-tuning to truly work for you.

### Real World Analogy

Prompting = Giving a note to a waiter each time: “No onions in my dish.”

Fine-tuning = Training the waiter to remember your preference every time you visit.

Prompting works for one-time or varied tasks.
Fine-tuning is better when you want consistency, efficiency, or specialization.

## Finals thoughts on why fine tuning is better than prompting

1. Domain Adaption: If your data is domain-specific (e.g., medical, legal, finance), prompts may not be enough to make the model behave well. Fine-tuning makes it fluent in your domain.
2. Behavior Control: You want the model to act in a specific way: Always polite, Always output JSON, Follow certain formats strictly. Prompting might fail inconsistently. Fine-tuning bakes the behavior in.
3. Efficiency at Scale: If you’re doing millions of requests, long prompts cost more (tokens = money). Fine-tuned models need shorter prompts and are faster to run.
4. RAG + Fine-Tuning: Even in Retrieval-Augmented Generation (RAG), fine-tuning can:
   - Improve grounding
   - Help model reason better with the documents
   - Reduce hallucinations
5. Few-Shot is Expensive, Fine-Tune is Scalable: Prompting with 5–10 examples ("few-shot") eats up context length. Fine-tuning is like giving those examples once, and then the model learns them forever.

---

# When and When NOT Should You Do Fine Tuning

### When You SHOULD Do Fine-Tuning
1. Your use case is highly domain-specific
2. You need consistent formatting or output
3. You want to embed your company's tone or behavior
4. You need fast, repeated generation for automation
5. Your prompt is too long or context limit is tight

### When You SHOULD NOT Fine-Tune
1. Your task works well with prompting or RAG
2. Your dataset is small pr poor quality
3. You need flexibility across many tasks
4. Your compute budget is low

---

# Types of Fine Tuning
For fine tuning any base model we have multiple ways, they are:
- Full Fine Tuning
- PEFT (Parameter-Efficient-Fine-Tuning)

Now let's see all of these one-by-one.

## Full Fine Tuning
Full fine-tuning means you update all the parameters(weights) of a pre-trained model using your own dataset. 

Think of it like this: You have a giant neural network — say, GPT-2 or LLaMA-2, which may have billions of parameters (weights). When you fine-tune it fully, you don't just tweak a few settings — you literally go inside and re-train the entire brain of the model using your domain-specific data. Analogy: Imagine you’ve hired a doctor who already knows everything about general medicine, but now you want them to become a cardiology expert in your hospital. Instead of just teaching them cardiology, you re-train them on the entire medical curriculum, including your cardiology notes. That’s full fine-tuning — expensive and intensive.

### How It Works

+ You load the pre-trained model.
+ You feed it your custom dataset.
+ You train the entire model end-to-end.
+ All the weights — from embeddings to transformer layers — are updated.

### Types of FFT
+ Supervised Fine Tuning (SFT)
+ Instruction Tuning
+ Reinforcement Learning from Human Feedback)
+ Domain Adaptation Fine Tuning

### Pros:
+ Can result in high performance for your specific task.
+ The model deeply adapts to your domain.
+ Works well when your data is large and diverse.

### Cons:
+ Requires LOTS of GPU and RAM (often 16GB+ VRAM minimum).
+ High cost in time and compute.
+ Risk of catastrophic forgetting (model forgets general knowledge).
+ Harder to maintain or update later.

## PEFT(Paramter Efficient Fine Tuning)
PEFT is a set of techniques that allow you to fine tune only a small part of a large pre-trained model instead of all it's parameters. PEFT is a smarter, lightweight way to fine-tune large language models (LLMs) — without updating all their billions of parameters.

Imagine this: Instead of retraining a whole massive model like GPT or LLaMA (which is costly and slow), PEFT says:
> “Let me freeze most of the brain, and just train a small add-on.”
This saves GPU, time, money, and reduces the risk of ruining the model's existing knowledge.

Goal: Low cost fine tuning with high performance.

### Why PEFT?
Fine tuning massive models needs huge GPUs, lots of memory, time and cost. But PEFT methods use less memory, require less data, work on consumer GPU. PEFT gives you:
+ Faster training
+ Less forgetting of original knowledge base
+ Smaller storage
+ Way cheaper

### How PEFT works:
In short, instead of updating all parameters, PEFT methods:
+ Freeze the base model (no updates).
+ Add small trainable modules or vectors or variables on top.
+ Only train those extra modules (often just <1% of the model).

### Common PEFT techniques
+ LoRA (Low Rank Adaptation) 
+ QLoRA (Quantized Low Rank Adaptation)
+ Prefix Tuning
+ Adapter Tuning
+ Prompt Tuning
+ P-Tuning / P-Tuning v2
+ IA³ (Input, Attention, and Activation scaling)

### Analogy
Consider fine tuning as painting.
+ Pretrained Models = a finished painting
+ Full Fine Tuning = Repainting the whole canvas
+ PEFT = just sticking a small transparent sheet on top with minor corrections.
When you look at the full painting again, its the original + corrections.

### When to use PEFT
Use PEFT when:
+ You want to customize an LLM for your domain (e.g., medical, legal, finance).
+ Your GPU or compute budget is limited.
+ You have a moderate dataset (few thousand to few hundred thousand samples).
+ You need multiple variants of a model without retraining the whole thing.

---

# Popular Tools & Libraries for Fine Tuning
+ Hugging Face Transformers [https://huggingface.co/docs/transformers/en/training]
+ PEFT Library (by Hugging Face)[https://huggingface.co/docs/peft/en/task_guides/prompt_based_methods]
+ LoRA (via PEFT)[https://huggingface.co/docs/peft/en/package_reference/lora]
+ Axolotl for full stacks[https://axolotl.ai/]
+ QLoRA for quantized training[https://arxiv.org/abs/2305.14314]

---


