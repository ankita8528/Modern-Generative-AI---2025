# Fine Tuning an LLM
In this blog we are going to cover the folowing topics:

1. What is Fine Tuning?
2. Why do we need Fine Tuning?
3. Difference between Fine Tuning and RAG
4. Fine Tuning vs Prompt Engineering vs RAG
5. When and When NOT to do Fine Tuning?
6. Types of Fine Tuning
7. Hands on pactice

---

***Disclaimer: If you are unable to navigate to any of the links provided in this blog, just understand that those are the upcoming blogs on topics which are huge and may take time to complete. This is my keen request to you all that please bear this inconvinience for a short span of time. I will surely update on [LinkedIn](https://www.linkedin.com/in/ankita-mishra-675b97230/) once all the links are active.***

---

# What is Fine Tuning?
Suppose you have a pre-trained model like GPT-4 or GPT-3.5. Now imagine this model as a super-smart student who has already read the entire internet — but has never read your specific textbook.

Let’s say you want this student to fully understand your book — whether it's on medicine, engineering, or any other specialized domain.

Now you have two options:

Option A: You teach them everything about medicine from scratch.
Option B: You simply teach them your book to help them grasp your specific content.

If you ask me, I’d go with Option B — because this student already knows a lot about medicine from the internet. You don’t need to start over. You’re just giving them your specialized context.

This saves your time, energy, and the student’s effort as well.

In the same way, when we want GPT-4 to become an expert in law and legal systems, we don’t need to retrain it from scratch on massive legal data. Instead, we fine-tune it on a smaller, domain-specific dataset, which gives it the additional understanding it needs — saving time, compute, and cost.

**Fine-tuning is taking a pre-trained model and training it a little more on your custom dataset so it performs better for your task.** During fine-tuning, the model's internal parameters — its weights and biases — are updated based on your data. This is the same principle by which all deep learning models learn. Example: ChatGPT - fine tuned on the top GPT base models using carefully curated conversational data, enabling it to better understand and respond like a helpful assistant.

**Lifecycle of Fine Tuning a LLM**

[Define Problem] → [Collect & Prepare Data] → [Choose Strategy & Model] → [Fine-Tune] → [Evaluate] → [Deploy] → [Monitor & Improve]

---

# Why do we need Fine Tuning?
Now you might be wondering that we can make the LLM to act as an expert in Law-practices with the help of prompting, so why do we need to fine tune specifically. Well here's the answer to your "why". We will understand this with the help of a story.

## Prompting vs Fine-Tuning — What’s the Difference?

Imagine you’ve hired a brilliant AI student — let’s call her Maya. Maya has read all the books on the internet, seen every YouTube tutorial, and studied every Wikipedia article. She's super smart. Now, you want Maya to help you write medical reports for a hospital.

**Day 1: You try prompting (giving her instrauctions)**
You say: “Hey Maya, every time I give you patient data, write a report in this exact format. Include vitals, diagnosis, suggested tests, and > always end with ‘Reviewed by Dr. Sharma’.” Maya nods and does a decent job. But the next day, you give her a new case, and she forgets the format. She forgets to mention the doctor's name. Sometimes she writes in bullet points. Sometimes in full paragraphs. So you keep repeating the same instructions: “Format like this. End with Dr. Sharma. Use technical terms.” It’s like writing her a sticky note every single time.

**Day 5 — You're Tired**
You're exhausted. You’ve written the same long prompt 50 times. And sometimes Maya still messes up the style or forgets medical abbreviations. You think: "She’s brilliant — but I wish she’d just learn how we do things here.” So You Fine-Tune Her (train her a bit). You spend a weekend with Maya. You show her hundreds of past medical reports from your hospital. You walk her through the patterns, the terminology, the format. She studies your data closely. You don’t need to teach her everything about medicine — she already knows that. You’re just showing her how you want things done. This is fine-tuning. Now, Maya doesn’t need sticky notes anymore.

**Week Later — She’s Fluent**
Now Maya produces flawless medical reports in seconds. She uses your preferred vocabulary, follows your templates, and never forgets “Reviewed by Dr. Sharma.”

She understands the vibe. She’s customized. She's fast. And you no longer need to remind her how to behave.

### Moral of the Story

***Prompting*** is like giving ***instructions*** to a brilliant but forgetful assistant — works for short-term, but not always reliable.

***Fine-tuning*** is like ***training*** that assistant to work your way, so she just knows what to do — permanently.

So even though Maya is a genius who read the whole internet, she still needs fine-tuning to truly work for you.

### Real World Analogy

Prompting = Giving a note to a waiter each time: “No onions in my dish.”

Fine-tuning = Training the waiter to remember your preference every time you visit.

Prompting works for one-time or varied tasks.
Fine-tuning is better when you want consistency, efficiency, or specialization.

## Finals thoughts on why fine tuning is better than prompting

1. Domain Adaption: If your data is domain-specific (e.g., medical, legal, finance), prompts may not be enough to make the model behave well. Fine-tuning makes it fluent in your domain.
2. Behavior Control: You want the model to act in a specific way: Always polite, Always output JSON, Follow certain formats strictly. Prompting might fail inconsistently. Fine-tuning bakes the behavior in.
3. Efficiency at Scale: If you’re doing millions of requests, long prompts cost more (tokens = money). Fine-tuned models need shorter prompts and are faster to run.
4. RAG + Fine-Tuning: Even in Retrieval-Augmented Generation (RAG), fine-tuning can:
   - Improve grounding
   - Help model reason better with the documents
   - Reduce hallucinations
5. Few-Shot is Expensive, Fine-Tune is Scalable: Prompting with 5–10 examples ("few-shot") eats up context length. Fine-tuning is like giving those examples once, and then the model learns them forever.

---

# Differnce between Fine Tuning and RAG
Imagine you’re training two different assistants for a company that deals with legal documents.

- For the **first assistant**, you give them months of training. You make them read every law book, contract, and legal clause until they can generate full legal drafts from memory. They don’t need internet or books anymore — they just *know* it. This is **Fine-Tuning**.

- For the **second assistant**, you skip the training. Instead, you give them access to a massive legal library. Every time they’re asked a question, they look it up instantly and respond with the right clause or rule. This is **RAG** (Retrieval-Augmented Generation).

Both are smart — but they serve different needs.

- One is deeply trained for consistency.
- The other is quick and up-to-date thanks to retrieval.

### Summary

- **Fine-Tuning** means training the model itself — it "learns" your data and stores it in its weights.
- **RAG** means keeping the model unchanged and helping it "look up" answers from an external source.
  
Use **Fine-Tuning** when:
- You need specialized, reliable, repeatable behavior.

Use **RAG** when:
- You need dynamic knowledge that changes often or is too large to train into the model.

---

# RAG vs Fine-Tuning vs Prompt Engineering

| **Aspect**             | **Prompt Engineering**                                              | **Fine-Tuning**                                                                   | **RAG (Retrieval-Augmented Generation)**                                         |
|------------------------|---------------------------------------------------------------------|------------------------------------------------------------------------------------|----------------------------------------------------------------------------------|
| **What it does**       | Crafts better prompts to get desired output                        | Retrains the model on domain/task-specific data to change internal behavior       | Adds external documents at inference time without changing model parameters     |
| **Model weights**      | No change                                                           | Changes model weights based on training data                                       | Model stays frozen; uses external retriever                                     |
| **Cost**               | No training cost, fastest                                           | High compute cost (especially for full fine-tuning)                               | Moderate cost (setup for retriever + index)                                     |
| **Data source**        | Relies on prompt content only                                       | Learns from your labeled dataset                                                  | Retrieves relevant content from external source                                 |
| **Ideal use case**     | Quick tasks, prototyping, experimentation                          | Domain-specific tasks (e.g., legal, medical, code generation)                    | Dynamic tasks where knowledge changes or is too large to fit into model         |
| **Output control**     | Medium control                                                      | High control — consistent and reliable                                            | Medium control — relies on quality of retrieval                                 |
| **Knowledge scope**    | Whatever is in the base model                                       | Injects additional permanent domain knowledge                                     | Can access up-to-date or large corpora on the fly                               |
| **Example**            | "Summarize this article in bullet points."                         | Train GPT on legal contracts for legal drafting                                   | Ask a bot to answer questions from uploaded company PDFs                        |

---

# When and When NOT Should You Do Fine Tuning

### When You SHOULD Do Fine-Tuning
1. Your use case is highly domain-specific
2. You need consistent formatting or output
3. You want to embed your company's tone or behavior
4. You need fast, repeated generation for automation
5. Your prompt is too long or context limit is tight

### When You SHOULD NOT Fine-Tune
1. Your task works well with prompting or RAG
2. Your dataset is small pr poor quality
3. You need flexibility across many tasks
4. Your compute budget is low

---

# Types of Fine Tuning
For fine tuning any base model we have multiple ways, they are:
- Full Fine Tuning
- PEFT (Parameter-Efficient-Fine-Tuning)

Now let's see all of these one-by-one.

## Full Fine Tuning
Full fine-tuning means you update all the parameters(weights) of a pre-trained model using your own dataset. 

Think of it like this: You have a giant neural network — say, GPT-2 or LLaMA-2, which may have billions of parameters (weights). When you fine-tune it fully, you don't just tweak a few settings — you literally go inside and re-train the entire brain of the model using your domain-specific data. Analogy: Imagine you’ve hired a doctor who already knows everything about general medicine, but now you want them to become a cardiology expert in your hospital. Instead of just teaching them cardiology, you re-train them on the entire medical curriculum, including your cardiology notes. That’s full fine-tuning — expensive and intensive.

### How It Works

+ You load the pre-trained model.
+ You feed it your custom dataset.
+ You train the entire model end-to-end.
+ All the weights — from embeddings to transformer layers — are updated.

### Types of FFT
+ [Supervised Fine Tuning (SFT)](https://medium.com/@ankitamishra8528/supervised-fine-tuning-teaching-ai-like-youd-teach-a-human-7f5c7e04d778)
+ [Instruction Tuning]([https://www.geeksforgeeks.org/artificial-intelligence/instruction-tuning-for-large-language-models/](https://medium.com/@ankitamishra8528/instruction-tuning-the-key-to-making-models-follow-you-better-c7fe57746121))

### Pros:
+ Can result in high performance for your specific task.
+ The model deeply adapts to your domain.
+ Works well when your data is large and diverse.

### Cons:
+ Requires LOTS of GPU and RAM (often 16GB+ VRAM minimum).
+ High cost in time and compute.
+ Risk of catastrophic forgetting (model forgets general knowledge).
+ Harder to maintain or update later.

## PEFT(Paramter Efficient Fine Tuning)
PEFT is a set of techniques that allow you to fine tune only a small part of a large pre-trained model instead of all it's parameters. PEFT is a smarter, lightweight way to fine-tune large language models (LLMs) — without updating all their billions of parameters.

Imagine this: Instead of retraining a whole massive model like GPT or LLaMA (which is costly and slow), PEFT says:
> “Let me freeze most of the brain, and just train a small add-on.”
This saves GPU, time, money, and reduces the risk of ruining the model's existing knowledge.

Goal: Low cost fine tuning with high performance.

### Why PEFT?
Fine tuning massive models needs huge GPUs, lots of memory, time and cost. But PEFT methods use less memory, require less data, work on consumer GPU. PEFT gives you:
+ Faster training
+ Less forgetting of original knowledge base
+ Smaller storage
+ Way cheaper

### How PEFT works:
In short, instead of updating all parameters, PEFT methods:
+ Freeze the base model (no updates).
+ Add small trainable modules or vectors or variables on top.
+ Only train those extra modules (often just <1% of the model).

### Common PEFT techniques
+ [LoRA (Low Rank Adaptation)](https://www.youtube.com/watch?v=l5a_uKnbEr4&list=PLZoTAELRMXVN9VbAx5I2VvloTtYmlApe3&index=2) *...most popular now*
+ [Prefix Tuning](https://medium.com/@razavipour6/understanding-prefix-tuning-a-novel-approach-to-fine-tuning-language-models-dc7dafeb32e4)
+ [Prompt Tuning](https://www.geeksforgeeks.org/artificial-intelligence/prompt-tuning/)
+ [You can find more PEFT techniques here](https://huggingface.co/blog/samuellimabraz/peft-methods)

### Analogy
Consider fine tuning as painting.
+ Pretrained Models = a finished painting
+ Full Fine Tuning = Repainting the whole canvas
+ PEFT = just sticking a small transparent sheet on top with minor corrections.
When you look at the full painting again, its the original + corrections.

### When to use PEFT
Use PEFT when:
+ You want to customize an LLM for your domain (e.g., medical, legal, finance).
+ Your GPU or compute budget is limited.
+ You have a moderate dataset (few thousand to few hundred thousand samples).
+ You need multiple variants of a model without retraining the whole thing.

### Pros
+ Extremely Resource Efficient: You only train a small number of parameters, which reduces GPU memory usage, compute cost, training time. Perfect for teams without access to powerful hardware.
+ Lightweight Models: PEFT methods like LoRA or Adapters can be stored as small "delta" files instead of saving the full model. You can switch tasks or domains by just loading different adapters — no need to retrain the whole model.
+  Reusability: You can keep the base model frozen and reuse it across multiple domains, e.g.: One adapter for medical QA, One for legal chatbots, One for retail.
+  Prevents Catastrophic Forgetting: Because the base model isn't altered, it retains its original general knowledge. Great for adding domain knowledge without destroying existing capabilities.
+   Great for Experimentation: Training is faster → you can iterate quickly on: Hyperparameters, Prompt formats, Dataset variations, Ideal for research, prototyping, or fast deployment cycles.

### Cons
+ Limited Capacity for Major Changes: PEFT can’t massively change the behavior of a model. If you need the model to deeply understand a new language or complex reasoning skill, full fine-tuning might be required.
+ Lower Ceiling for Task Performance: On very complex or low-resource tasks, PEFT may underperform compared to full fine-tuning.
+ Fragmented Support in Some Frameworks: Not all models or inference tools (especially proprietary ones) support plug-and-play LoRA or Adapter modules.
+ Evaluation Complexity: Comparing the effectiveness of different PEFT techniques can be tricky. Also, evaluating mixed adapter performance across multiple tasks can get messy.
+ Still Needs Quality Data: PEFT reduces training cost but doesn’t eliminate the need for clean, high-quality, task-specific datasets. Garbage in → garbage out still applies.

---

# Popular Tools & Libraries for Fine Tuning
+ Hugging Face Transformers
+ PEFT Library
+ LoRA (via PEFT)
+ QLoRA
+ Llama.cpp + GGUF models
+ OpenLLM
+ Axolotl
+ Colossal AI

## Other Fine Tuning Techniques
+ [Quantized Fine Tuning (QLoRA)](https://medium.com/@dillipprasad60/qlora-explained-a-deep-dive-into-parametric-efficient-fine-tuning-in-large-language-models-llms-c1a4794b1766)
+ [BitFit](#) ---coming soon
+ [Multitask Fine Tuning](#) ---coming soon

There is a vast varity of fine tuning techniques, you don't need to study all neither for interviews nor for work. Whatever necessary, more than that has been already covered in this blog.

---

### You can find a Colab Notebook for LLM Fine Tuning Practice
[Hands-On Practice on LLM Fine Tuning](https://colab.research.google.com/drive/1bpNxH8nDKGc2IRfewgnNwHsqRKiFiIpU) ---coming soon

*I understand and respect that your time is limited, and while this project may take some time to complete, it’s being designed in a way that allows you to simply run and understand the hands-on fine-tuning process without getting lost in setup or complexity.*

Stay tuned for more! 
