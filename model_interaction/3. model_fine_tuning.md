# Fine Tuning an LLM
In this blog we are going to cover the folowing topics:

1. What is Fine Tuning?
2. Why do we need Fine Tuning?
3. When should we use it?
4. Types of Fine Tuning
5. When and When NOT to use Fine Tuning?
6. Some real world usecases
7. Hands on pactice.

---
# What is Fine Tuning?
Suppose you have a pre-trained model like GPT-4 or GPT-3.5. Now imagine this model as a super-smart student who has already read the entire internet — but has never read your specific textbook.

Let’s say you want this student to fully understand your book — whether it's on medicine, engineering, or any other specialized domain.

Now you have two options:

Option A: You teach them everything about medicine from scratch.
Option B: You simply teach them your book to help them grasp your specific content.

If you ask me, I’d go with Option B — because this student already knows a lot about medicine from the internet. You don’t need to start over. You’re just giving them your specialized context.

This saves your time, energy, and the student’s effort as well.

In the same way, when we want GPT-4 to become an expert in law and legal systems, we don’t need to retrain it from scratch on massive legal data. Instead, we fine-tune it on a smaller, domain-specific dataset, which gives it the additional understanding it needs — saving time, compute, and cost.

**Fine-tuning is taking a pre-trained model and training it a little more on your custom dataset so it performs better for your task.** Example - ChatGPT fine tuned on GPT4.

---

# Why do we need Fine Tuning if we have techniques like Prompt Engineering and RAG?

