# Fine Tuning an LLM
In this blog we are going to cover the folowing topics:

1. What is Fine Tuning?
3. Why do we need Fine Tuning?
4. When should we do it?
5. Types of Fine Tuning
6. When and When NOT to use Fine Tuning?
7. Some real world usecases
8. Hands on pactice.

---
# What is Fine Tuning?
Suppose you have a pre-trained model like GPT-4 or GPT-3.5. Now imagine this model as a super-smart student who has already read the entire internet ‚Äî but has never read your specific textbook.

Let‚Äôs say you want this student to fully understand your book ‚Äî whether it's on medicine, engineering, or any other specialized domain.

Now you have two options:

Option A: You teach them everything about medicine from scratch.
Option B: You simply teach them your book to help them grasp your specific content.

If you ask me, I‚Äôd go with Option B ‚Äî because this student already knows a lot about medicine from the internet. You don‚Äôt need to start over. You‚Äôre just giving them your specialized context.

This saves your time, energy, and the student‚Äôs effort as well.

In the same way, when we want GPT-4 to become an expert in law and legal systems, we don‚Äôt need to retrain it from scratch on massive legal data. Instead, we fine-tune it on a smaller, domain-specific dataset, which gives it the additional understanding it needs ‚Äî saving time, compute, and cost.

**Fine-tuning is taking a pre-trained model and training it a little more on your custom dataset so it performs better for your task.** And while you will train the model on your custom dataset, the weights and the biases (aka parameters) of the model will change, this is how any deep learning model is being trained. Example: ChatGPT - fine tuned on the top GPT base models.

---

# Why do we need Fine Tuning?
Now you might be wondering that we can make the LLM to act as an expert in Law-practices with the help of prompting, so why do we need to fine tune specifically. Well here's the answer to your "why". We will understand this with the help of a story.

## üß† Prompting vs Fine-Tuning ‚Äî What‚Äôs the Difference?

Imagine you‚Äôve hired a brilliant AI student ‚Äî let‚Äôs call her Maya. Maya has read all the books on the internet, seen every YouTube tutorial, and studied every Wikipedia article. She's super smart. Now, you want Maya to help you write medical reports for a hospital.

**Day 1: You try prompting (giving her instrauctions)**
You say: ‚ÄúHey Maya, every time I give you patient data, write a report in this exact format. Include vitals, diagnosis, suggested tests, and > always end with ‚ÄòReviewed by Dr. Sharma‚Äô.‚Äù Maya nods and does a decent job. But the next day, you give her a new case, and she forgets the format. She forgets to mention the doctor's name. Sometimes she writes in bullet points. Sometimes in full paragraphs. So you keep repeating the same instructions: ‚ÄúFormat like this. End with Dr. Sharma. Use technical terms.‚Äù It‚Äôs like writing her a sticky note every single time.

**Day 5 ‚Äî You're Tired**
You're exhausted. You‚Äôve written the same long prompt 50 times. And sometimes Maya still messes up the style or forgets medical abbreviations. You think: "She‚Äôs brilliant ‚Äî but I wish she‚Äôd just learn how we do things here.‚Äù So You Fine-Tune Her (train her a bit). You spend a weekend with Maya. You show her hundreds of past medical reports from your hospital. You walk her through the patterns, the terminology, the format. She studies your data closely. You don‚Äôt need to teach her everything about medicine ‚Äî she already knows that. You‚Äôre just showing her how you want things done. This is fine-tuning. Now, Maya doesn‚Äôt need sticky notes anymore.

**Week Later ‚Äî She‚Äôs Fluent**
Now Maya produces flawless medical reports in seconds. She uses your preferred vocabulary, follows your templates, and never forgets ‚ÄúReviewed by Dr. Sharma.‚Äù

She understands the vibe. She‚Äôs customized. She's fast. And you no longer need to remind her how to behave.

### Moral of the Story

***Prompting*** is like giving ***instructions*** to a brilliant but forgetful assistant ‚Äî works for short-term, but not always reliable.

***Fine-tuning*** is like ***training*** that assistant to work your way, so she just knows what to do ‚Äî permanently.

So even though Maya is a genius who read the whole internet, she still needs fine-tuning to truly work for you.

### Real World Analogy

Prompting = Giving a note to a waiter each time: ‚ÄúNo onions in my dish.‚Äù

Fine-tuning = Training the waiter to remember your preference every time you visit.

Prompting works for one-time or varied tasks.
Fine-tuning is better when you want consistency, efficiency, or specialization.

## Finals thoughts on why fine tuning is better than prompting

1. Domain Adaption: If your data is domain-specific (e.g., medical, legal, finance), prompts may not be enough to make the model behave well. Fine-tuning makes it fluent in your domain.
2. Behavior Control: You want the model to act in a specific way: Always polite, Always output JSON, Follow certain formats strictly. Prompting might fail inconsistently. Fine-tuning bakes the behavior in.
3. Efficiency at Scale: If you‚Äôre doing millions of requests, long prompts cost more (tokens = money). Fine-tuned models need shorter prompts and are faster to run.
4. RAG + Fine-Tuning = üî•: Even in Retrieval-Augmented Generation (RAG), fine-tuning can:
   - Improve grounding
   - Help model reason better with the documents
   - Reduce hallucinations
5. Few-Shot is Expensive, Fine-Tune is Scalable: Prompting with 5‚Äì10 examples ("few-shot") eats up context length. Fine-tuning is like giving those examples once, and then the model learns them forever.

---

# When and When NOT Should You Do Fine Tuning

## ‚úÖ When You SHOULD Do Fine-Tuning
1. Your use case is highly domain-specific
2. You need consistent formatting or output
3. You want to embed your company's tone or behavior
4. You need fast, repeated generation for automation
5. Your prompt is too long or context limit is tight

## ‚ùå When You SHOULD NOT Fine-Tune
1. Your task works well with prompting or RAG
2. Your dataset is small pr poor quality
3. You need flexibility across many tasks
4. Your compute budget is low

---

# Types of Fine Tuning
For fine tuning any base model we have multiple ways, they are:
- Full Fine Tuning
- PEFT (Parameter-Efficient-Fine-Tuning)
- Prompt Based Fine Tuning
- Quantized Fine Tuning

Now let's see all of these one-by-one.

## Full Fine Tuning
Full fine-tuning means you update all the parameters(weights) of a pre-trained model using your own dataset. 

Think of it like this: You have a giant neural network ‚Äî say, GPT-2 or LLaMA-2, which may have billions of parameters (weights). When you fine-tune it fully, you don't just tweak a few settings ‚Äî you literally go inside and re-train the entire brain of the model using your domain-specific data. Analogy: Imagine you‚Äôve hired a doctor who already knows everything about general medicine, but now you want them to become a cardiology expert in your hospital. Instead of just teaching them cardiology, you re-train them on the entire medical curriculum, including your cardiology notes. That‚Äôs full fine-tuning ‚Äî expensive and intensive.

### How It Works

+ You load the pre-trained model.
+ You feed it your custom dataset.
+ You train the entire model end-to-end.
+ All the weights ‚Äî from embeddings to transformer layers ‚Äî are updated.

### ‚úÖ Pros:
+ Can result in high performance for your specific task.
+ The model deeply adapts to your domain.
+ Works well when your data is large and diverse.

### ‚ùå Cons:
+ Requires LOTS of GPU and RAM (often 16GB+ VRAM minimum).
+ High cost in time and compute.
+ Risk of catastrophic forgetting (model forgets general knowledge).
+ Harder to maintain or update later.

## PEFT(Paramter Efficient Fine Tuning)
PEFT is a set of techniques that allow you to fine tune only a small part of a large pre-trained model instead of all it's parameters. PEFT is a smarter, lightweight way to fine-tune large language models (LLMs) ‚Äî without updating all their billions of parameters.

Imagine this: Instead of retraining a whole massive model like GPT or LLaMA (which is costly and slow), PEFT says:
> ‚ÄúLet me freeze most of the brain, and just train a small add-on.‚Äù
This saves GPU, time, money, and reduces the risk of ruining the model's existing knowledge.

Goal: Low cost fine tuning with high performance.

### Why PEFT?
Fine tuning massive models needs huge GPUs, lots of memory, time and cost. But PEFT methods use less memory, require less data, work on consumer GPU. PEFT gives you:
+ Faster training
+ Less forgetting of original knowledge base
+ Smaller storage
+ Way cheaper

### How PEFT works:
In short, instead of updating all parameters, PEFT methods:
+ Freeze the base model (no updates).
+ Add small trainable modules or vectors or variables on top.
+ Only train those extra modules (often just <1% of the model).

### Common PEFT techniques
+ LoRA(Low Rank Adaption) Fine Tuning
+ QLoRA(Quantized Low Rank Adaption) Fine Tuning
